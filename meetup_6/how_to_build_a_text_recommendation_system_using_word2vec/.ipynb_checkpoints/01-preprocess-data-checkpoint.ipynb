{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing steps:\n",
    "\n",
    "## 1. Process Rick and Morty subtitles\n",
    "## 2. Clean data by:\n",
    "### (I.) lemmatise words\n",
    "### (II.) remove stopwords\n",
    "### (III.) clean nonalphabetic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "from unidecode import unidecode\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "import srt\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enlarge pandas columns with for a better display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain subtitle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = 'data/rick_and_morty'\n",
    "\n",
    "files = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if file.endswith('.srt') and 'ipynb_checkpoints' not in os.path.join(r, file):\n",
    "            files.append(os.path.join(r, file))\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open file as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/simpsons_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['raw_character_text', 'spoken_words'], dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158314, 2)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2879</td>\n",
       "      <td>Homer Simpson</td>\n",
       "      <td>27850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3879</td>\n",
       "      <td>Marge Simpson</td>\n",
       "      <td>13172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>Bart Simpson</td>\n",
       "      <td>12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3567</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>10756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>C. Montgomery Burns</td>\n",
       "      <td>3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4066</td>\n",
       "      <td>Moe Szyslak</td>\n",
       "      <td>2808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5506</td>\n",
       "      <td>Seymour Skinner</td>\n",
       "      <td>2385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4315</td>\n",
       "      <td>Ned Flanders</td>\n",
       "      <td>2056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2599</td>\n",
       "      <td>Grampa Simpson</td>\n",
       "      <td>1802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1193</td>\n",
       "      <td>Chief Wiggum</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       raw_character_text  spoken_words\n",
       "2879        Homer Simpson         27850\n",
       "3879        Marge Simpson         13172\n",
       "648          Bart Simpson         12995\n",
       "3567         Lisa Simpson         10756\n",
       "944   C. Montgomery Burns          3077\n",
       "4066          Moe Szyslak          2808\n",
       "5506      Seymour Skinner          2385\n",
       "4315         Ned Flanders          2056\n",
       "2599       Grampa Simpson          1802\n",
       "1193         Chief Wiggum          1790"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['raw_character_text'])['spoken_words'].count().reset_index().sort_values(by='spoken_words', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk to him. He didn't touch my lesson plan. What did he teach you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end of recess. Now, just in case any of you have decided to put any thought into this, we'll have our final statements. Martin?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                                                                                                                                               spoken_words\n",
       "0              Miss Hoover              No, actually, it was a little of both. Sometimes when a disease is in all the magazines and all the news shows, it's only natural that you think you have it.\n",
       "1             Lisa Simpson                                                                                                                                                     Where's Mr. Bergstrom?\n",
       "2              Miss Hoover                                                                I don't know. Although I'd sure like to talk to him. He didn't touch my lesson plan. What did he teach you?\n",
       "3             Lisa Simpson                                                                                                                                                 That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end of recess. Now, just in case any of you have decided to put any thought into this, we'll have our final statements. Martin?"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **cleaning**: _Lemmatizes and removes stopwords_\n",
    "#### **cleaning**: _Removes sentences with less than 2 words (Word2Vec uses context words to learn the vector representation of a target word)_\n",
    "#### **clean_nonalphabetic**: _Removes words with nonalphabetic characters_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  dog and dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser'])\n",
    "\n",
    "def cleaning(doc):\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt).strip()\n",
    "\n",
    "def clean_nonalphabetic(row):\n",
    "    return re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the preprocessing procedure while using spacy as NLP pipeline, to apply a series of algorihms such as: \n",
    "#### tokenisation, lemmatisation and custom preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.99 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "cleaning_data = ((clean_nonalphabetic(str(row))) for row in df['spoken_words'])\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(cleaning_data, batch_size=5000, n_threads=12)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a new dataframe with cleansed dialogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.DataFrame({\"spoken_words\": txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86834, 1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv(\"data/processed_simpsons_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
