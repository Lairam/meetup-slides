{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a text recommendation system using Word2vec\n",
    "***\n",
    "\n",
    "## **Table of contents**\n",
    "\n",
    "### 1. Introduction to Natural Language Processing and Word2Vec\n",
    "    \n",
    "### 2. Data preprocessing\n",
    "    \n",
    "### 3. Data Insights\n",
    "    \n",
    "### 4. Train model\n",
    "   \n",
    "### 5. Analyse trained model\n",
    "\n",
    "### **Note**\n",
    "### Download data from: https://www.kaggle.com/pierremegret/dialogue-lines-of-the-simpsons and place it in __data__ folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Natural Language Processing and Word2Vec\n",
    "\n",
    "\n",
    "Natural language processing (NLP) is an area of computer science and artificial intelligence that is known to be concerned with the interaction between computer and humans in natural language. It can be seen as a crossdomain between Artificial Intelligence and linguistics.\n",
    "\n",
    "\n",
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "Word embedding applications:\n",
    "\n",
    "1. Compute similar words\n",
    "2. Create a group of related words\n",
    "3. Feature for text classification\n",
    "4. Document clustering\n",
    "\n",
    "***\n",
    "### How word embedding words ? \n",
    "\n",
    "#### Let's take the following example: **I love cheese but I hate chocolate**\n",
    "\n",
    "![](./docs/word2vec_example.png)\n",
    "\n",
    "### I = [0, 1, 0, 1, 1, 0]\n",
    "### love = [1, 0, 1, 0, 0, 0]\n",
    "### cheese = [0, 1, 0, 1, 0, 0]\n",
    "### ...\n",
    "\n",
    "***\n",
    "\n",
    "#### Vector space projection\n",
    "![alt text](./docs/word2vec_space.png \"Word2Vec space projection\")\n",
    "## man is to woman as *King* is to *Queen*\n",
    "\n",
    "## king +(woman−man) ≈ queen\n",
    "***\n",
    "\n",
    "\n",
    "## Word2Vec architecture\n",
    "\n",
    "##### (context, target) = ([i, cheese], love), ([love, but], cheese), ([cheese, I], but), ...\n",
    "\n",
    "#### **CBOW** - predicting the word given its context (given this set of context words, what missing word is likely to also appear at the same time?)\n",
    "#### **Skip-gram** - predicting the context given a word (given this single word, what are the other words that are likely to appear near it at the same time?)\n",
    "\n",
    "![alt text](./docs/word2vec_arch.png \"Word2Vec space projection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)\n",
    "\n",
    "\n",
    "**Additional information**\n",
    "1. Spacy\n",
    "2. Textblob ([Documentation](https://textblob.readthedocs.io/en/dev/)) - you get most of the info from their documentation (quite well made, although there is no specific information about the trained models and what dataset was used for them. The nice part is that you can extend them with your own corpus as presented in the documentation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
